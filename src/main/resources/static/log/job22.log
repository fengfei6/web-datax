
DataX (DATAX-OPENSOURCE-3.0), From Alibaba !
Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.


2020-04-02 09:41:43.114 [main] INFO  VMInfo - VMInfo# operatingSystem class => sun.management.OperatingSystemImpl
2020-04-02 09:41:43.128 [main] INFO  Engine - the machine info  => 

	osInfo:	Oracle Corporation 1.8 25.60-b23
	jvmInfo:	Linux amd64 3.10.0-862.el7.x86_64
	cpu num:	1

	totalPhysicalMemory:	-0.00G
	freePhysicalMemory:	-0.00G
	maxFileDescriptorCount:	-1
	currentOpenFileDescriptorCount:	-1

	GC Names	[Copy, MarkSweepCompact]

	MEMORY_NAME                    | allocation_size                | init_size                      
	Eden Space                     | 273.06MB                       | 273.06MB                       
	Code Cache                     | 240.00MB                       | 2.44MB                         
	Survivor Space                 | 34.13MB                        | 34.13MB                        
	Compressed Class Space         | 1,024.00MB                     | 0.00MB                         
	Metaspace                      | -0.00MB                        | 0.00MB                         
	Tenured Gen                    | 682.69MB                       | 682.69MB                       


2020-04-02 09:41:43.154 [main] INFO  Engine - 
{
	"content":[
		{
			"reader":{
				"name":"mysqlreader",
				"parameter":{
					"column":[
						"*"
					],
					"connection":[
						{
							"jdbcUrl":[
								"jdbc:mysql://192.144.129.188:3306/test"
							],
							"table":[
								"film"
							]
						}
					],
					"password":"******",
					"username":"root"
				}
			},
			"writer":{
				"name":"mysqlwriter",
				"parameter":{
					"column":[
						"*"
					],
					"connection":[
						{
							"jdbcUrl":"jdbc:mysql://39.97.255.156:3307/lp_test",
							"table":[
								"film"
							]
						}
					],
					"password":"**********",
					"username":"root",
					"writeMode":"insert"
				}
			}
		}
	],
	"setting":{
		"errorLimit":{
			"percentage":0.02,
			"record":0
		},
		"speed":{
			"channel":3
		}
	}
}

2020-04-02 09:41:43.189 [main] WARN  Engine - prioriy set to 0, because NumberFormatException, the value is: null
2020-04-02 09:41:43.190 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0
2020-04-02 09:41:43.191 [main] INFO  JobContainer - DataX jobContainer starts job.
2020-04-02 09:41:43.193 [main] INFO  JobContainer - Set jobId = 0
2020-04-02 09:41:43.718 [job-0] INFO  OriginalConfPretreatmentUtil - Available jdbcUrl:jdbc:mysql://192.144.129.188:3306/test?yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true.
2020-04-02 09:41:43.719 [job-0] WARN  OriginalConfPretreatmentUtil - 您的配置文件中的列配置存在一定的风险. 因为您未配置读取数据库表的列，当您的表字段个数、类型有变动时，可能影响任务正确性甚至会运行出错。请检查您的配置并作出修改.
2020-04-02 09:41:44.542 [job-0] INFO  OriginalConfPretreatmentUtil - table:[film] all columns:[
FId,FContent,FAuthor,FCategory,FPrice,FName,FDate,FImage
].
2020-04-02 09:41:44.542 [job-0] WARN  OriginalConfPretreatmentUtil - 您的配置文件中的列配置信息存在风险. 因为您配置的写入数据库表的列为*，当您的表字段个数、类型有变动时，可能影响任务正确性甚至会运行出错。请检查您的配置并作出修改.
2020-04-02 09:41:44.544 [job-0] INFO  OriginalConfPretreatmentUtil - Write data [
insert INTO %s (FId,FContent,FAuthor,FCategory,FPrice,FName,FDate,FImage) VALUES(?,?,?,?,?,?,?,?)
], which jdbcUrl like:[jdbc:mysql://39.97.255.156:3307/lp_test?yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true]
2020-04-02 09:41:44.544 [job-0] INFO  JobContainer - jobContainer starts to do prepare ...
2020-04-02 09:41:44.544 [job-0] INFO  JobContainer - DataX Reader.Job [mysqlreader] do prepare work .
2020-04-02 09:41:44.545 [job-0] INFO  JobContainer - DataX Writer.Job [mysqlwriter] do prepare work .
2020-04-02 09:41:44.545 [job-0] INFO  JobContainer - jobContainer starts to do split ...
2020-04-02 09:41:44.546 [job-0] INFO  JobContainer - Job set Channel-Number to 3 channels.
2020-04-02 09:41:44.548 [job-0] INFO  JobContainer - DataX Reader.Job [mysqlreader] splits to [1] tasks.
2020-04-02 09:41:44.549 [job-0] INFO  JobContainer - DataX Writer.Job [mysqlwriter] splits to [1] tasks.
2020-04-02 09:41:44.578 [job-0] INFO  JobContainer - jobContainer starts to do schedule ...
2020-04-02 09:41:44.588 [job-0] INFO  JobContainer - Scheduler starts [1] taskGroups.
2020-04-02 09:41:44.590 [job-0] INFO  JobContainer - Running by standalone Mode.
2020-04-02 09:41:44.622 [taskGroup-0] INFO  TaskGroupContainer - taskGroupId=[0] start [1] channels for [1] tasks.
2020-04-02 09:41:44.626 [taskGroup-0] INFO  Channel - Channel set byte_speed_limit to -1, No bps activated.
2020-04-02 09:41:44.627 [taskGroup-0] INFO  Channel - Channel set record_speed_limit to -1, No tps activated.
2020-04-02 09:41:44.658 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started
2020-04-02 09:41:44.661 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Begin to read record by Sql: [select * from film 
] jdbcUrl:[jdbc:mysql://192.144.129.188:3306/test?yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true].
2020-04-02 09:41:44.729 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Finished read record by Sql: [select * from film 
] jdbcUrl:[jdbc:mysql://192.144.129.188:3306/test?yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true].
2020-04-02 09:41:45.011 [0-0-0-writer] WARN  CommonRdbmsWriter$Task - 回滚此次写入, 采用每次写入一行方式提交. 因为:Data truncation: Data too long for column 'FContent' at row 1
2020-04-02 09:41:45.035 [0-0-0-writer] ERROR StdoutPluginCollector - 
com.mysql.jdbc.MysqlDataTruncation: Data truncation: Data too long for column 'FContent' at row 1
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3876) ~[mysql-connector-java-5.1.38.jar:5.1.38]
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3814) ~[mysql-connector-java-5.1.38.jar:5.1.38]
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2478) ~[mysql-connector-java-5.1.38.jar:5.1.38]
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2625) ~[mysql-connector-java-5.1.38.jar:5.1.38]
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2551) ~[mysql-connector-java-5.1.38.jar:5.1.38]
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861) ~[mysql-connector-java-5.1.38.jar:5.1.38]
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1192) ~[mysql-connector-java-5.1.38.jar:5.1.38]
	at com.alibaba.datax.plugin.rdbms.writer.CommonRdbmsWriter$Task.doOneInsert(CommonRdbmsWriter.java:382) [plugin-rdbms-util-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.plugin.rdbms.writer.CommonRdbmsWriter$Task.doBatchInsert(CommonRdbmsWriter.java:362) [plugin-rdbms-util-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.plugin.rdbms.writer.CommonRdbmsWriter$Task.startWriteWithConnection(CommonRdbmsWriter.java:297) [plugin-rdbms-util-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.plugin.rdbms.writer.CommonRdbmsWriter$Task.startWrite(CommonRdbmsWriter.java:319) [plugin-rdbms-util-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.plugin.writer.mysqlwriter.MysqlWriter$Task.startWrite(MysqlWriter.java:78) [mysqlwriter-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.core.taskgroup.runner.WriterRunner.run(WriterRunner.java:56) [datax-core-0.0.1-SNAPSHOT.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]
2020-04-02 09:41:45.038 [0-0-0-writer] ERROR StdoutPluginCollector - 脏数据: 
{"exception":"Data truncation: Data too long for column 'FContent' at row 1","record":[{"byteSize":1,"index":0,"rawData":1,"type":"LONG"},{"byteSize":3,"index":1,"rawData":"asd","type":"STRING"},{"byteSize":3,"index":2,"rawData":"asd","type":"STRING"},{"byteSize":3,"index":3,"rawData":"asd","type":"STRING"},{"byteSize":3,"index":4,"rawData":"asd","type":"STRING"},{"byteSize":3,"index":5,"rawData":"asd","type":"STRING"},{"byteSize":2,"index":6,"rawData":11,"type":"LONG"},{"byteSize":8,"index":7,"rawData":1585670400000,"type":"DATE"}],"type":"writer"}
2020-04-02 09:41:45.063 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[416]ms
2020-04-02 09:41:45.063 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] completed it's tasks.
2020-04-02 09:41:54.646 [job-0] INFO  StandAloneJobContainerCommunicator - Total 1 records, 26 bytes | Speed 2B/s, 0 records/s | Error 1 records, 26 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%
2020-04-02 09:41:54.647 [job-0] ERROR JobContainer - 运行scheduler 模式[standalone]出错.
2020-04-02 09:41:54.647 [job-0] ERROR JobContainer - Exception when job run
com.alibaba.datax.common.exception.DataXException: Code:[Framework-14], Description:[DataX传输脏数据超过用户预期，该错误通常是由于源端数据存在较多业务脏数据导致，请仔细检查DataX汇报的脏数据日志信息, 或者您可以适当调大脏数据阈值 .].  - 脏数据条数检查不通过，限制是[0]条，但实际上捕获了[1]条.
	at com.alibaba.datax.common.exception.DataXException.asDataXException(DataXException.java:26) ~[datax-common-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.core.util.ErrorRecordChecker.checkRecordLimit(ErrorRecordChecker.java:58) ~[datax-core-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.core.job.scheduler.AbstractScheduler.schedule(AbstractScheduler.java:89) ~[datax-core-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.core.job.JobContainer.schedule(JobContainer.java:535) ~[datax-core-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.core.job.JobContainer.start(JobContainer.java:119) ~[datax-core-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.core.Engine.start(Engine.java:92) [datax-core-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.core.Engine.entry(Engine.java:171) [datax-core-0.0.1-SNAPSHOT.jar:na]
	at com.alibaba.datax.core.Engine.main(Engine.java:204) [datax-core-0.0.1-SNAPSHOT.jar:na]
2020-04-02 09:41:54.648 [job-0] INFO  StandAloneJobContainerCommunicator - Total 1 records, 26 bytes | Speed 26B/s, 1 records/s | Error 1 records, 26 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%
2020-04-02 09:41:54.648 [job-0] ERROR Engine - 

经DataX智能分析,该任务最可能的错误原因是:
com.alibaba.datax.common.exception.DataXException: Code:[Framework-14], Description:[DataX传输脏数据超过用户预期，该错误通常是由于源端数据存在较多业务脏数据导致，请仔细检查DataX汇报的脏数据日志信息, 或者您可以适当调大脏数据阈值 .].  - 脏数据条数检查不通过，限制是[0]条，但实际上捕获了[1]条.
	at com.alibaba.datax.common.exception.DataXException.asDataXException(DataXException.java:26)
	at com.alibaba.datax.core.util.ErrorRecordChecker.checkRecordLimit(ErrorRecordChecker.java:58)
	at com.alibaba.datax.core.job.scheduler.AbstractScheduler.schedule(AbstractScheduler.java:89)
	at com.alibaba.datax.core.job.JobContainer.schedule(JobContainer.java:535)
	at com.alibaba.datax.core.job.JobContainer.start(JobContainer.java:119)
	at com.alibaba.datax.core.Engine.start(Engine.java:92)
	at com.alibaba.datax.core.Engine.entry(Engine.java:171)
	at com.alibaba.datax.core.Engine.main(Engine.java:204)

